<!DOCTYPE html>
<html lang="it">
  <head>
    <meta http-equiv="content-type" content="text/html; charset=windows-1252">
    <link href="style.css" rel="stylesheet" type="text/css">
    <link href="animate.css" rel="stylesheet" type="text/css">
    <title>Water Detection</title>
    <script src="jquery-3.4.1.js" type="text/javascript"></script>
    <script src="scripts.js" type="text/javascript"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
    <script type="text/javascript">
    c=1;
    d=2;
    img_video=[30,150];
    var a;
    function inizialization(){
      c=1;
      d=2;
      document.getElementById("immagine").src="dataset/ter"+d+"_"+c+".png";
      document.getElementById("immagine1").src="dataset/ter"+d+"_"+c+"-annotation.png";
     var x = document.getElementById("selectimage");
        while (x.length > 0) {x.remove(x.length-1);}
        var x = document.getElementById("selectvideo");
        while (x.length > 0) {x.remove(x.length-1);}
      for(i=0;i<img_video.length;i++){
        var option=document.createElement("option");
        option.text="video "+(i+1);
        option.value=i;
        document.getElementById("selectvideo").add(option);
      }
      for(i=0;i<img_video[0];i++){
        var option=document.createElement("option");
        option.text="immagine "+(i+1);
        option.value=i;
        document.getElementById("selectimage").add(option);
      }
      document.getElementById("selectvideo").selectedIndex=0;
      document.getElementById("selectimage").selectedIndex=0;


    }


    function velo(){

    }

    function changevideo(){
      var x = document.getElementById("selectimage");
      while (x.length > 0) {x.remove(x.length-1);}
        switch(document.getElementById("selectvideo").value){
          case "0":
          for(i=0;i<img_video[0];i++){
            d=2;
            changeimage()
            var option=document.createElement("option");
            option.text="immagine "+(i+1);
            option.value=i;
            x.add(option);}
            break;
          case "1":
          d=1;
          changeimage()
            for(i=0;i<img_video[1];i++){
            var option=document.createElement("option");
            option.text="immagine "+(i+1);
            option.value=i;
            x.add(option);}
            break;
        }
    document.getElementById("selectimage").selectedIndex=0;
    }


    function avanti(){
      if (c+1>selectimage.length){
        c=1;
      }
      else{
        c++
      }

      document.getElementById("immagine1").src="dataset/ter"+d+"_"+c+"-annotation.png";
      document.getElementById("immagine").src="dataset/ter"+d+"_"+c+".png";
      document.getElementById('selectimage').selectedIndex=c-1;
      clearInterval(myTimer);
      }




    function indietro(){
      if (c-1==0){
        c=selectimage.length;
      }
      else{
        c--;
      }
      document.getElementById("immagine1").src="dataset/ter"+d+"_"+c+"-annotation.png";
      document.getElementById("immagine").src="dataset/ter"+d+"_"+c+".png";
      document.getElementById('selectimage').selectedIndex=c-1;
      clearInterval(myTimer);

    }


    function changeimage(){
      c=document.getElementById('selectimage').value;
      c++;
      document.getElementById("immagine1").src="dataset/ter"+d+"_"+c+"-annotation.png";
      document.getElementById("immagine").src="dataset/ter"+d+"_"+c+".png";
    }

    function Download(){
      window.open("http://download1348.mediafire.com/jkvqeh3k6r3g/c54dg2l21u71280/dataset.zip");
    }

    </script>
  </head>
  <body onload="inizialization()">
    <div id="top" class="item animated fadeIn">
      <video autoplay muted loop id="myVideo" position="center">
        <source src="immagini/VID_20191121_105937.mp4" type="video/mp4">
      </video>
      <div style="position:absolute;left:20px;top:10px;"><img src="immagini/logouni.png" width="300"></div>
      <div id="titolo"><h1 style="font-family: NordicaThin;  font-size: 5em;color:white;"><u>Water Detection</u></h1></div>
      <div id="sottotitolo" style="position:relative;top:0px;"><h2 style="font-family: NordicaHairline;color:white;">Riconoscimento ostacoli tramite reti neurali</h2></div>
    </div>
    <div id="box_indice">
      <div id="indice" >
      <div style="position:absolute;left:0%;width:15%;height:50px;text-align:center;line-height:50px;" ><a href="#tool"  class="titoli">TOOL DI SEGMENTAZIONE</a></div>
      <div style="position:absolute;left:15%;width:8%;height:50px;text-align:center;line-height:50px" ><a href="#dataset"  class="titoli">DATASET</a></div>
      <div style="position:absolute;left:22%;width:15%;height:50px;text-align:center;line-height:50px" ><a href="#archi_rete"  class="titoli">ARCHITETTURA RETE</a></div>
      <div style="position:absolute;left:36%;width:8%;height:50px;text-align:center;line-height:50px" ><a href="#addestramento"  class="titoli">TRAINING</a></div>
      <div style="position:absolute;left:43%;width:8%;height:50px;text-align:center;line-height:50px;" ><a href="#Testing"  class="titoli">TESTING</a></div>
      <div  style="position:absolute;left:50%;width:10%;height:50px;text-align:center;line-height:50px;"><a href="#Conclusioni" class="titoli">CONCLUSIONI</a></div>
      <div style="position:absolute;right:5%;width:8%;" ><img src="immagini/logouni.png" width="100%"></div>
    </div></div>
    <div id="Intro">
        <br><br><p>
        L'automatizzazione di droni &egrave; un obbiettivo sempre pi&ugrave; diffuso.
        I droni possono facilitare il compito a utenti che ne facciano uso, esperti e non, in tantissimi settori, migliorandone le qualit&agrave; lavorative.
        Un ambito in cui l'automatizzazione pu&ograve; essere sicuramente utile e vantaggiosa &egrave; il monitoraggio delle acque.
        Attualmente tale monitoraggio avviene tramite sensori statici o tramite prelievi manuali da parte di utenti, operazioni che possono risultare difficili e a volte anche poco accurate a causa delle correnti acquatiche che spostano i fattori di interesse.<br>

        Un'alternativa valida a questa metodologia non del tutto affidabile consiste nell'utilizzo di droni acquatici per il campionamento dell'acqua, obbiettivo che si intende raggiungere con il progetto INTCATCH 2020 che prevede la realizzazione di AVS (Autonomoud Veichle Systems).
        Gli AVS, che verranno implementati entro gennaio 2020, avranno lo scopo di rendere il monitoraggio delle acque un'operazione efficiente e sostenibile in quanto andrebbero a colmare le lacune insite nel campionamento manuale.
        Gli AVS montano sensori per il rilevamento di parametri dell'acqua come Temperatura, Ph e Ossigeno disciolto, finalizzati a recepire dati durante il movimento del drone.
        Attualmente tale movimento del drone pu&ograve; essere controllato tramite radiocomando o autonomo per mezzo di una particolare app con cui si pu&ograve; impostare il percorso da effettuare.
        Durante gli spostamenti automatici dei droni esiste un problema non di poco conto: il rilevamento di ostacoli.
        Questo perch&egrave; il drone ad oggi non &egrave; ancora in grado di riconoscere oggetti presenti nel suo percorso rendendo cos&igrave; lo spostamento autonomo di scarsa utilit&agrave;.<br>
      </p><p>
        In questa pagina presentiamo uno studio effettuato dall'universit&agrave; di Verona, per il progetto INTCATCH, sul rilavemento di ostacoli in superficie.
        Il rilevamento &egrave; visivo e avviene grazie a videocamere montate sopra ai droni. I video verranno sottoposti in tempo reale a reti neurali per la classificazione binaria dei singoli frame, campo in cui hanno avuto molto impatto.
        Nel nostro caso vengono classificati i pixel in due classi: acqua e non-acqua. Una volta classificati, i vari frame passeranno ad algortmi specifici che ne calcoleranno la "linea di costa".
        Questa linea servir&agrave; a delimitare lo spazio di movimento del drone cos&igrave; che potr&agrave; muoversi di conseguenza evitando gli eventuali ostacoli.
      </p><p>
        Una rete neurale prima di essere caricata sul drone deve essere addestrata al suo compito.
        Nel nostro caso per addestrare la rete neurale bisogna darle in input un dataset appositamente creato. Nel dataset sono presenti immagini dell'ambiente considerato ad ognuna delle quali corrisponde una maschera.
        Una maschera &egrave; anch'essa un'immagine ed ha il compito di etichettare la classe di appartenenza dei pixel, tramite una notazione posizionale, dell'immagine ad essa associata.
        La rete, processando un'immagine alla volta, potra confrontare il risultato ottenuto con la maschera ed aggiustare i vari pesi di ogni neurone al fine di avere un percentuale di precisione ottimizzata.
        Nel progetto dell'universit&agrave; ho portato il mio contributo tramite la creazione di una parte del dataset e piccoli cambiamenti nel tool di segmentazione per riuscire a riconoscere una pi&ugrave; ampia tipologia di ostacoli e di ambienti di utilizzo. Infine &egrave; stata addestrata la rete ottenendo il risultato atteso.
      </p><p>
        Lo scopo finale nel progetto &egrave; fornire uno strumento di riconoscimento di immagini tramite reti neurali evitando cos&igrave; l'utilizzo di algoritmo supervisionati e non.
          Il progetto ha ottenuto ottimi risultati superando una precisione di classificazione superiore al 99%.
           Il metodo quindi risulta una buona alternativa ai classici algoritmi usati per il riconoscimento.
        Il problema che si riscontra tramite questo tipo di approccio &egrave; la velocit&agrave; con cui viene processato il video:
         per questo motivo verranno proposte pi&ugrave; architetture di rete ognuna con i suoi pro e contro.      </p>
        <br><br>
        <!--La detection viene fatta attraverso il training di una rete neurale. Le reti neurali sono uno strumento informatico nato durante gli anni '50 ma sviluppatosi soltanto negli ultimi anni grazie alle tecnologie avanzate e alle grandi potenze di calcolo sviluppate. Una rete neurale è composta da tanti strati di centinaia/migliaia di neuroni i quali ognuno ha lo scopo di calcolare una sommatoria pesata dei dati che arrivano dalle varie sinapsi e passarle allo strato successivo.
        Per la fase di training verrà dato in input un grande numero di frame gia etichettati cosi che, grazie all'aiuto di specifiche tecniche e calcoli (ad esempio algoritmi specifici di backPropagation e funzioni di costo), possa aggiustare i propri parametri al fine di rinoscere al meglio la natura di ogni pixel. Per creare il dataset bisogna etichettare ogni pixel di ogni frame disponibile da video pertinenti. Per fare ciò bisogna creare una nuova immagine (una mask) da dare come etichetta all'immagine originale in cui a ogni pixel viene associata una classe di appartenenza.
        set di immagini ognuna associata ad una maschera appositamente creata in cui ad ogni posizione di pixel viene associata una particolare classe di appartenenza
        -->
    </div>
    <div style="position:relative;width:50%;left:25%;top:20px;" class="img_didasc">
    <video id="video" width="100%" autoplay loop muted >
    <source src="VID_20191121_105937.mp4" type="video/mp4" >Il video non è supportato
    </video>
    <p id="didasc"> Vid. 1: Presentazione del movimento in acqua del drone</p>
    </div>

    <div style="position:relative; top :30px; background-color:transparent;" >
      <h1 style="color:white;background-color:rgba(0,0,0,0.5);;font-family: NordicaThin;color:white; ">Tool di segmentazione</h1>
      <div id="tool">
        <br><br>
        <p>

          Il primo passo verso il nostro scopo &egrave; stato creare il dataset di immagini con relative maschere. Per le immagini sono stati estratti vari frame provenienti da video appositamente registrati e utilizzate immagini provenienti da dataset pubblici. Nello specfico sono stati usati video registrati al lago di Garda e sul fiume Ter in Spagna, mentre come daset pubblici inerenti all'argomento sono state usate le immagini disponibili al sito <a href="https://osf.io/edq4b/" style="text-decoration:none;color:orange;">https://osf.io/edq4b/</a> . Per la creazione delle maschere invece &egrave; stato usato un tool dedicato.<br>Il <a href="#immagine_tool" style="color:orange;text-decoration:none;">Video 2<a> mostra un esempio di creazione di una maschera. Il tool utilizzato &egrave; scaricabile al sito <a href="https://github.com/ZenoMontolli/segmentation_tool" style="color:orange;text-decoration:none;">https://github.com/ZenoMontolli/segmentation_tool</a>. <br>
          Una volta caricata l'immagine, bisogna inizialmente segmentarla come meglio si gradisce.
          Con questo programma &egrave; possibile segmentare l'immagine con tre diversi metodi presenti nella libreria Skimage di Python:<br>
          <ul>
            <li> <b>SLIC</b> (Simple Linear Iterative Clustering): Segmenta l'immagine usando l'algoritmo di clustering k-means nello spazio dei colori (x,y,z). </li>
            <li> <b>Felzenszwalb</b>: Produce una sovra-segmentazione di un'immagine multicanale (i.e RGB) utilizzando un clustering rapido e minimo basato su spanning tree sulla griglia dell'immagine. Il parametro sigma &egrave; il diametro di un kernel Gaussiano, usato per fare smoothing dell'immagine prima della segmentazione.</li>
            <li> <b>Quickshift</b>: Produce una sovra-segmentazione dell'immagine usando l'algoritmo di ricerca quickshift. Questo algoritmo, pur essendo molto efficacie, rende la segmentazione molto lenta.</li>
          </ul>
          Per una spiegazione dettagliata sull'utilizzo dei vari metodi utilizzati si rimanda al sito della <a href="https://scikit-image.org/docs/0.12.x/api/skimage.segmentation.html#id3" style=" text-decoration:none;color:orange">libreria Python Skimage</a>.
        </p><p>
        Una volta segmentata l'immagine &egrave; possibile etichettare le regioni di pixel con una specifica classe. Nel nostro caso sono state utilizzate 5 superclassi (animali in acqua, ostacoli in acqua, cielo, costa e acqua) per raggruppare oggetti di natura simile in questo contesto.
        Ogni etichetta &egrave; associata ad un colore (consultabili nella <a href="#legenda" style="text-decoration: none;color:orange">legenda</a>) cos&igrave; che, una volta che si vuole concludere l'operazione di labeling, viene creata una maschera di dimensioni uguali all'immagine di partenza in cui ogni pixel colorato corrisponde all'etichetta del pixel dell'immagine di partenza.<br>
      <br>Una parte delle immagini del dataset con le corrispondenti maschere sono consultabili nella sezione <a href="#dataset" style="text-decoration: none;color:orange">Dataset</a> con la possibilit&agrave; di fare Download.
    </p><p>Durante l'addestramento della rete &egrave; possibile anche lavorare sul dataset con l'operazione di data Augmentation. Questa tecnica permette di aumentare artificialmente le dimensioni del dataset usando operazioni come insversione, rotazione e cambiamenti di luminosit&agrave; sulle sottoimmagini dell'immagine originale. L'ampliamento del dataset pu&ograve; aumentare la capacit&agrave; di generalizzazione della rete.<br><br><br><br>
      </div></div>
      <div style="position:relative; top :30px;" >
      <div id="prova" >
        <div id="immagine_tool" class="img_didasc">
          <video id="video" width="100%" autoplay loop muted >
          <source src="make_img.mp4" type="video/mp4" >Il video non è supportato
          </video>
          <p id="didasc">Vid. 2: Esempio di creazione di un'immagine per il dataset tramite il tool di segmentazione.</p>
        </div>
        <div id="legenda"> <b>LEGENDA COLORI</b>:
          <dl>
            <dt><span class="dot1" style="position:absolute;height:10px;width:10px;background-color:#6464FF;display:inline;border-radius:50%"><dd>acqua</dd></span></td>
            <dt><span class="dot2" style="position:absolute;height:10px;width:10px;background-color:#00FF00;display:inline;border-radius:50%;"></span></dt>
              <dd>costa</dd>
            <dt><span class="dot3" style="position:absolute;height:10px;width:10px;background-color:#FF00FF;display:inline;border-radius:50%"></span></dt>
              <dd>cielo</dd>
            <dt><span class="dot4" style="position:absolute;height:10px;width:10px;background-color:#FF0000;display:inline;border-radius:50%"></span></dt>
              <dd>ostacoli in acqua</dd>
            <dt><span class="dot5" style="position:absolute;height:10px;width:10px;background-color:#C8C814;display:inline;border-radius:50%"></span></dt>
              <dd>animali in acqua</dd>
          </dl>
        </div>
      </div>

    </div>
    <div class="box" id ="dataset">
      <h1 style="color:white;background-color:rgba(0,0,0,0.5);;font-family: NordicaThin;color:white;">Dataset</h1>
    </div>
    <div class="box">
      <button id="bottoneDownload" onclick="Download()">Download</button>
    </div>
    <div class="box">
      <select id="selectvideo" onchange="changevideo()">
      </select>
      <select id="selectimage" onchange="changeimage()">
      </select>
      <a href="javascript:inizialization()" class="testo">reset</a>
    </div>
    <div id="box_immagini" class="item animated zoomIn">
      <div id="im" ><img id="immagine" class="immagine"> <a id="bottone1" onclick="indietro()"></a></div>
      <div id="im1"><img id="immagine1" class="immagine" > <a id="bottone2"onclick="avanti()"></a> </div>
    </div>
    <div style="position :relative;"></div>

  <div style="position:relative;top:50px;color:black;padding-left:5%; width:80%;left:5%;padding-right:5%;background-color:white">

    <br><p>Una volta completata la creazione del dataset, quest'ultimo deve essere suddiviso casualmente in 3 sottoinsiemi. Ogn sottoinsieme servir&agrave; per una determinata fase di creazione di una rete neurale. Le tre fasi sono:
      <ul><li><b>Training</b></li>
        <li><b>Validation</b></li>
        <li><b>Testing</b></li>
      </ul>
      Prima di vedere il funzionamento della rete andiamo ad analizzarne l'architettura.
    </p><br>
  </div>
  <h1 style="position:relative; top: 50px;color:white;background-color:rgba(0,0,0,0.5);;font-family: NordicaThin;color:white;">Architettura Rete</h1>
  <div  id="archi_rete">
    <div style="position:relative;left:0%;width:50%;">
    <br><br>
    <p>Come architettura di rete &egrave; stata usata una CNN (Convolutional Neural Network). Questo tipo di architettura denota buone prestazioni per la classificazione di immagini. Pi&ugrave; precisamente si tratta di un'architettura che discende da U-net per la segmentazione delle immagini.<br>
      U-net &egrave; stata sviluppata da Olaf Ronneberger et al. per la Bio Image Segmentation. Essa prevede due fasi: la prima &egrave; chiamta di "encoding" e serve per acquisire il contesto nell'immagine riducendone le dimensioni. Questo percorso &egrave; una successione tradizionale di strati convoluzionali e di max-pooling. La seconda fase &egrave; chiamata decoding e viene utilizzata per abilitare la localizzazione precisa utilizzando convoluzioni trasposte e ricomponendo le dimensioni dell'immagine originale. Questo tipo di rete  &egrave; completamente convoluzionale (FCN) end-to-end, cio &egrave; contiene solo livelli Convolutional e non contiene alcun livello Dense con conseguente assenza di restrizioni sulle dimensioni dell'immagini in ingresso.
    </p><p>
      La <a href="#immagine_archi" style="color:orange;text-decoration:none;">Fig. 1</a> presenta la U-net modificata utilizzata, chiamata anche Full BN 160&#x2715;160.<br>
      A differenza della U-net originale lo strato di codifica utilizzato &egrave; composto da quattro livelli anzich&egrave; cinque e vengono estratte la met&agrave; delle feature per ogni livello. La prima fase &egrave; composta da otto strati convoluzionali 3&#x2715;3 e da tre operazioni di max pooling 2&#x2715;2 con stride 2. La fase di espansione &egrave; costituita da sei strati convoluzionali 3&#x2715;3 e da tre strati di trasposizione 2&#x2715;2. La mappa a 32 canali estratta nell'ultimo passo della rete viene proiettata in uno spazio a singolo canale tramite una funzione di attivazione sigmoide per generare la probabilit&agrave; di appartenenza alla classe "acqua" per ogni signolo pixel.<br>
       In questa architettura &egrave; stata introdotta anche una funzione chiamata Batch Normalization. Viene introdotta questa funzione per far fronte al fenomeno chiamato internal covariate shift. Con questo termine si intende il fatto che ogni strato di neuroni si deve continuamente adattare ad una distribuzione diversa derivata dagli output dello strato precedente. Quindi la BN migliora la qualit&agrave; e la velocit&agrave; di apprendimento fissando la distribuzione tramite un'operazione di normalizzazione calcolando media e varianza sul gruppo di input chiamato Batch.
     </p></div>
     <div style="position:relative;left:0%;width:100%;">
      Oltre all'architettura Full BN 160&#x2715;160 sono state proposte altre configurazioni. L'architettura infatti deve essere in grado di elaborare i dati con gran velocit&agrave; per riuscire a riconoscere ostacoli in un tempo accettabile. Questo il motivo per cui sono state introdotte altre tipi di architettura chiamate Half-Conv BN 160&#x2715;160, Full mobile-net-v1-layer 160&#x2715;160 e Full mobile-net -v2-layer 160&#x2715;160.
      Half-Conv BN 160&#x2715;160 contiene la met&agrave; dei livelli convoluzionali della rete Full BN. Riducendo i livelli convoluzionali a un singolo livello per codifica/decodifica, &egrave; possibile ottenere un calcolo pi&ugrave; rapido in termini di FPS a scapito della precisione di rilevazione. Inoltre, togliendo la BN, e sempre a svantaggio di una miglior precisione, si pu&ograve; ulteriormente migliorare la velocit&agrave; di calcolo. Ci riferiamo alle due versioni della rete senza BN come Full 160&#x2715;160 e Half-Conv 160&#x2715;160 , rispettivamente. Le reti Full mobile-net-v1-layer 160&#x2715;160 Full mobile-net -v2-layer 160&#x2715;160 derivano da MobileNet. MobileNet &egrave; un tipo di rete dove gli strati convoluzionali tradizionali dal livello 1 al 4 sono sostituiti da convoluzioni depthwise separable (DSC). Questo tipo di rete &egrave; apposito per dispositivi mobili perch&egrave; rende l'architettura molto pi&ugrave; leggera e portabile aumentandone la velocit&agrave; di calcolo.
    </p><br><br></div>
    <div id="immagine_archi" style="position:absolute;left:53%;width:40%;top:20%;" class="img_didasc" >
        <img src="architettura.jpg"/ style="width:100%">
        <p id="didasc">Fig. 1: Architettura U-net Full BN 160 x 160.</p>
      </div>
    </div>

  <h1 style="position:relative; top: 50px;color:white;background-color:rgba(0,0,0,0.5);;font-family: NordicaThin;color:white;">Training</h1>
  <div id="addestramento" style="position:relative;top:70px;color:black;padding-left:5%; padding-right:5%; background-color:white;left:5%;right:5%;width:80%">
    <div style="position:relative;left:0%;width:50%;">
      <br><br>
    <p>
      Una rete neurale artificiale &egrave; un insieme di strati di neuroni. Ogni neurone applica una funzione all'input, passato dal livello precedente, moltiplicato per un peso \(\theta\). Con un operazione di forward-propagation l'input parte dal primo strato e si propaga fino allo strato finale. Nel nostro caso l'immagine di input passa attraverso strati convoluzionali e di max-pooling per estrarre dallo strato di output una maschera di probabilit&agrave; di appartenenza dei pixel alla classe acqua. La fase di addestramento della rete consiste nell'aggiustare i vari pesi \(\theta\) tramite un'oprazione chiamata back-propagation di una funzione costo al fine di migliorarne le qualit&agrave; predittive.</p>
      <p>
      Per addestrare la rete neurale bisogna utilizzare il dataset in cui ad ogni immagine &egrave; associata una mask che ne indica le classi di appartenenza dei pixel. Inizialmente i parametri della rete sono impostati in modo casuale. Spetter&agrave; alla fase di training aggiustare questi parametri al fine di completare la funzionalit&agrave; richiestra. Per questo motivo bisogna impostare un algoritmo di ottimizzazione e una funzione di costo.
      La funzione di costo ha il compito di calcolare la distanza tra la predizione della rete e la maschera associata all'immagine di input. Una volta calcolata la funzione di costo &egrave; possibile utilizzare algoritmi di ottimizzazione che, utilizzandone il gradiente, bilanciano i pesi in modo da minimizzare l'errore.<br>
      In questo caso la loss function &egrave; stata trovata sperimentalmente tramite un confronto tra varie funzioni. La funzione che ha dato risultati migliori (come mostrato in <a href="#immagine_loss" style="text-decoration:none;color:orange" >Fig. 2</a>) &egrave; stata Dice Similarity Coefficient (DSC) definita come:
      $$J = {\scr L}_{DSC} = 1 - \frac{2\sum\nolimits_{i}^Np_ig_i}{\sum\nolimits_{i}^Np_i+\sum\nolimits_{i}^Ng_i}$$
      dove \(p_i,g_i&#x2208;[0,1]\) rappresentano rispettivamente i valori continui della griglia di predizione e il vero valore appertenente alla maschera, per ogni pixel \(i\). <br>
      Per quanto riguarda l'algoritmo di ottimizzazione nel nostro caso &egrave; stato usato la discesa lungo gradiente con Mini-batch. Un mini-batch &egrave; un sottoinsieme di immagini di dimensione \(m\) del dataset originale. Per ogni mini-batch, quindi, l'algoritmo calcola il gradiente della funzione costo. L'ottimizzazione su mini-batch utilizza la media ponderata dei gradienti calcolati all'interno del singolo batch per muovere i pesi alla ricerca del minimo della funzione risultante .

      </p></div>
        <div style="position:relative;left:0%;width:100%;">
          <p>
          Quindi la funzione calcolata su un mini-batch risulta essere una media del tipo:
          $$J(\theta,x_{(z:z+m)},y_{(z:z+m)})=\frac{1}{m}\sum\nolimits_{z=0}^mJ(\theta,x_{(z)},y_{(z)})$$
          Come ottimizzatore &egrave; stato usato Adam (Adaptive moment estimation) con batch size di 32 immagini. L'ottimizzatore Adam calcola la discesa lungo gradiente con momento. Il conseguente aggiornamento dei pesi pu&ograve; essere scritto come:
          $$\theta=\theta-\alpha\Delta J(\theta;x_{(z:z+m)};y_{(z:z+m)})$$
          dove a \(\theta\), definito come singolo peso del neurone, viene sottratto il gradiente \(\Delta\),calcolato tramite l'ottimizzatore, moltiplicato per un fattore \(\alpha\) chiamato learning Rate. Nel nostro caso \(\alpha\) &egrave; uguale a \(10^{-4}\).
          Il training della rete &egrave; stato fatto per 100 epoche. Per evitare l'overfitting &egrave; stato usata l'interruzione anticipata con patient 20 basata sulla loss function rispetto al validation set.</p><br><br>
        </div>
        <div id="immagine_loss" style="position:absolute;width:40%;left:53%;top:20%;" class="img_didasc" >
          <img src="img_loss.jpg"/ style="width:100%">
          <p id="didasc">Fig. 2: Confronto delle varie funzioni loss tramite metrica Dice/F1.</p>
        </div>
  </div>



  <h1 style="position:relative; top: 70px;color:white;background-color:rgba(0,0,0,0.5);;font-family: NordicaThin;color:white;">Testing</h1>
  <div id="Testing" style="position:relative;top:70px;color:black;padding-left:5%;padding-right:5%; background-color:white;left:5%;right:5%;width:80%;">
    <div>
      <br><br>
    <p>
      Per testare la qualit&agrave; di rilevazione delle reti sono stati considerati vari parametri. Questi parametri vengono calcolati con funzioni specifiche nell'intorno della linea di costa, luogo in cui avvengono con pi&ugrave; probabilità errori.
      Successivamente vengono elencati i vari parametri con le relative formule. Nelle formule vengono utilizzate le notazioni TP (True Positive), TN (True Negative), FP (Flase Positive), FN (False Negative).

      <ul>
        <li> <b>Precision (P)</b>   $$P = \frac{TP}{TP + FP}$$ </li>
        <li> <b>Recall (R)</b>:  $$R=\frac{TP}{TP + FN}$$</li>
        <li> <b>Accuracy (A)</b>:  $$A=\frac{TP + TN}{TP + FP + TN + FN}$$</li>
        <li> <b>F1-score (\(F_1\))</b>:  $$F_1=2\frac{P \times R}{P + R}$$</li>
      </ul>
    </p>
    <p>
      Nella<a href="#tabella_risultati" style="text-decoration:none;color:orange" > tabella 1</a> &egrave; possibile confrontare i vari parametri per le differenti architetture.
      Come si pu&ograve; notare l'architettura Full BN 160&#x2715;160 &egrave; la rete con i migliori parametri per il riconoscimento.
      Nelle colonne FPS CPU/GPU sono segnate le velocit&agrave; di elaborazione dei frame calcolati in media su 10 essecuzioni.
      Si pu&ograve; evidenziare da queste ultime due colonne che un'architettura pi&ugrave; snella come Half-Conv 160&#x2715;160 riesca ad aumentare il FPS a discapito di una miglior precisione di riconoscimento.<br>
      </div>
      <div style="text-align: center;width:90%; position: relative;left:5%;right:5%;" class="img_didasc">
        <table style="border-spacing: 20px;color:black; width:100%; border:1px solid #666; background:aliceblue" id="tabella_risultati">
        <thead style="font-weight: 900;">
          <tr><td>Rete</td><td>N parametri</td><td>P</td><td>R</td><td>A</td><td>\(F_1\)</td><td>FPS CPU</td><td>FPS GPU</td></tr>
        </thead>
        <tbody>
          <tr><td>Half-Conv 160&#x2715;160</td><td>948001</td><td>0.974</td><td>	0.983 </td><td>0.978</td><td>0.978</td><td style="font-weight: 900;">5.23</td><td style="font-weight: 900;">13.27</td></tr>
          <tr><td>Half-Conv BN 160&#x2715;160</td><td>950817</td><td>0.974</td><td>0.982</td><td>0.977</td><td>0.977</td><td>4.88</td><td>12.18</td></tr>
          <tr><td>Full 160&#x2715;160</td><td>1925601</td><td>0.994</td><td>0.980</td><td>0.988</td><td>0.986</td><td>3.51</td><td>10.53</td></tr>
          <tr><td>Full BN 160&#x2715;160</td><td>1931233</td><td>0.993</td><td style="font-weight: 900;">0.983</td><td style="font-weight: 900;">0.989</td><td style="font-weight: 900;">0.988</td><td>3.06</td><td>9.90</td></tr>
          <tr><td>Full mobile-net-v1-layer 160&#x2715;160</td><td style="font-weight: 900;">422465</td><td style="font-weight: 900;">0.996</td><td>0.972</td><td>0.986</td><td>0.983</td><td>4.25</td><td>9.85</td></tr>
        </tbody>
      </table>
    <p>tabella dei risultati</p></div><br><br>
</div>

<h1 style="position:relative; top:70px;color:white;background-color:rgba(0,0,0,0.5);;font-family: NordicaThin;color:white;">Conclusioni</h1>
<div style="position:relative;top:70px;color:black;padding-left:5%;padding-right:5%; background-color:white;left:5%;right:5%;width:80%;">
  <br><br><p>
    In questa pagina abbiamo fornito un metodo di creazione di una rete neurale per la classificazione binaria di immagini. Abbiamo fornito inoltre 5 architetture differenti utilizzabili in base alle esigenze del contesto.
    Una rete neurale infatti pu&ograve; essere utilizzata in svariati ambiti in cui si possono raggiungere risultati pi&ugrave; o meno accettabili. Nel nostro caso, dal punto di vista teorico, si pu&ograve; concludere che l'utilizzo della rete neurale &egrave; un'ottima soluzione al riconoscimento di oggetti in acqua rendendo l'automatizzazione dei droni acquatici un'operazione efficiente ed affidabile.
  </p><br><br>
</div>
<div style="position:relative;top:70px; background-color:rgba(0,0,0,0.5);width:100%;height:100px;">
</div>

  <a id="back_to_top"></a>

  </body>
</html>
