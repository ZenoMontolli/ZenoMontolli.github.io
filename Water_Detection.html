<!DOCTYPE html>
<html lang="it">
  <head>
    <meta http-equiv="content-type" content="text/html; charset=windows-1252">
    <link href="stile.css" rel="stylesheet" type="text/css">
    <link href="animate.css" rel="stylesheet" type="text/css">
    <title>IntCatch</title>
    <script src="jquery-3.4.1.js" type="text/javascript"></script>
    <script src="scripts.js" type="text/javascript"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
    <script type="text/javascript">
    c=1;
    d=1;
    img_video=[150,30];
    var a;
    function inizialization(){
      c=1;
      d=1;
      document.getElementById("immagine").src="dataset/ter"+d+"_"+c+".png";
      document.getElementById("immagine1").src="dataset/ter"+d+"_"+c+"-annotation.png";
     var x = document.getElementById("selectimage");
        while (x.length > 0) {x.remove(x.length-1);}
        var x = document.getElementById("selectvideo");
        while (x.length > 0) {x.remove(x.length-1);}
      for(i=0;i<img_video.length;i++){
        var option=document.createElement("option");
        option.text="video "+(i+1);
        option.value=i;
        document.getElementById("selectvideo").add(option);
      }
      for(i=0;i<img_video[0];i++){
        var option=document.createElement("option");
        option.text="immagine "+(i+1);
        option.value=i;
        document.getElementById("selectimage").add(option);
      }
      document.getElementById("selectvideo").selectedIndex=0;
      document.getElementById("selectimage").selectedIndex=0;


    }


    function changevideo(){
      var x = document.getElementById("selectimage");
      while (x.length > 0) {x.remove(x.length-1);}
        switch(document.getElementById("selectvideo").value){
          case "0":
          for(i=0;i<img_video[0];i++){
            d=1;
            changeimage()
            var option=document.createElement("option");
            option.text="immagine "+(i+1);
            option.value=i;
            x.add(option);}
            break;
          case "1":
          d=2;
          changeimage()
            for(i=0;i<img_video[1];i++){
            var option=document.createElement("option");
            option.text="immagine "+(i+1);
            option.value=i;
            x.add(option);}
            break;
        }
    document.getElementById("selectimage").selectedIndex=0;
    }


    function avanti(){
      if (c+1>selectimage.length){
        c=1;
      }
      else{
        c++
      }

      document.getElementById("immagine1").src="dataset/ter"+d+"_"+c+"-annotation.png";
      document.getElementById("immagine").src="dataset/ter"+d+"_"+c+".png";
      document.getElementById('selectimage').selectedIndex=c-1;
      clearInterval(myTimer);
      }


    function indietro(){
      if (c-1==0){
        c=selectimage.length;
      }
      else{
        c--;
      }
      document.getElementById("immagine1").src="dataset/ter"+d+"_"+c+"-annotation.png";
      document.getElementById("immagine").src="dataset/ter"+d+"_"+c+".png";
      document.getElementById('selectimage').selectedIndex=c-1;
      clearInterval(myTimer);

    }


    function changeimage(){
      c=document.getElementById('selectimage').value;
      c++;
      document.getElementById("immagine1").src="dataset/ter"+d+"_"+c+"-annotation.png";
      document.getElementById("immagine").src="dataset/ter"+d+"_"+c+".png";
    }

    function Download(){
      window.open("http://download1348.mediafire.com/jkvqeh3k6r3g/c54dg2l21u71280/dataset.zip");
    }

    </script>
  </head>
  <body onload="inizialization()">

    <div id="top" class="item animated fadeIn">
      <div id="titolo"><h1 style="; font-family: serif; font-size: 3em;">Water Detection</h1></div>
      <div id="indice"><a id="freccia_indice"></a>&nbsp;&nbsp;&nbsp;&nbsp;INDICE
      <div id="Menu"><br><a href="#Intro" style="text-decoration:none;color:black;">INTRODUZIONE</a><br>
                    <a href="#tool" style="text-decoration:none;color:black">TOOL DI SEGMENTAZIONE</a><br>
                    <a href="#dataset" style="text-decoration:none;color:black">DATASET</a><br>
                    <a href="#archi_rete" style="text-decoration:none;color:black">ARCHITETTURA RETE</a><br>
                    <a href="#addestramento" style="text-decoration:none;color:black">TRAINING</a><br>
                    <a href="#Testing" style="text-decoration:none;color:black">TESTING</a><br><br>
    </div></div>
    </div>

    <div id="Intro" style="position:relative;top :20px;color:white;padding-left:5%;padding-right:5%;">
      <h1>Introduzione</h1>
      <p>
        L'automatizzazione di droni &egrave; un obbiettivo che sta prendendo sempre pi&ugrave; popolarit&agrave;. Questi droni possono facilitare il compito a utenti, esperti e non, aumentandone le qulit&agrave; lavorative. Un ambito in cui questa automatizzazione pu&ograve; venire comodo &egrave; sicuramente il monitoraggio delle acque. Infatti il monitoraggio tramite sensori statici o tramite prelievi manuali da parte di utenti pu&ograve; risultare un lavoro difficile e a volte anche poco accurato a causa delle correnti acquatiche che spostano i fattori di interesse. Un'alternativa valida a questo problema &egrave; l'utilizzo di droni acquatici per il campionamento dell'acqua. Questo &egrave; l'obbiettivo che si &egrave; posto il progetto INTCATCH 2020. Il progetto nasce quindi per creare AVS (Autonomoud Veichle Systems) per il monitoraggio dell'acqua. Nella <a href="#immagine_boat" style="color:orange;text-decoration:none">Fig. 1</a> &egrave; mostrato un prototipo di AVS.  Questi AVS montano sensori per il rilevamento di fattori come Temperatura, Ph e Ossigeno disciolto. Il movimento pu&ograve; essere controllato o autonomo per mezzo di una particolare app con cui si pu&ograve; decidere il percorso da effettuare. Il problema principale dell'automatismo &egrave; il rilevamento degli ostacoli e il loro superamento durante gli spostamenti. Nell'ambito acquatico sono numerosi i fattori che ne aumentano le difficolt&agrave; di stabilit&agrave; quali riflessi, onde e luminosit&agrave; variabile.
        <p></p>
        In questa pagina presentiamo uno studio effettuato dall'universit&agrave; di Verona, per il progetto INTCATCH, sul rilavemento di ostacoli in superficie.
      </p><p>
        Concentreremo l'attenzione sul processo di riconoscimento della linea di galleggiamento e di ostacoli tramite l'utilizzo di reti neurali. Una rete neurale &egrave; un ottimo strumento per classificare immagini. Nel nostro caso vengono classificati i pixel di ogni singolo frame, provenienti da videocamere montate sopra i droni, in due classi: acqua e non-acqua. Una volta classificati i dati, il drone potr&agrave; muoversi di conseguenza evitando cos&igrave; ostacoli presenti nel suo percorso.
      </p><p>
        Una rete neurale prima di essere caricata sul drone deve essere addestrata a svolgere il suo compito e testata per valutarne il funzionamento. Per l'addestramento della rete neurale vengono utilizzate immagini provenienti da video appositamente registrati. La rete neurale una volta addestrata verr&agrave; testata su immagini provenienti da un dataset diverso da quello di training. Una volta caricata sul drone dovrà essere in grado di lavorare con i frame provenienti dalle videocamere per riconscere dal vivo la presenza di ostacoli e della linea di costa.
      </p>
        <!--La detection viene fatta attraverso il training di una rete neurale. Le reti neurali sono uno strumento informatico nato durante gli anni '50 ma sviluppatosi soltanto negli ultimi anni grazie alle tecnologie avanzate e alle grandi potenze di calcolo sviluppate. Una rete neurale è composta da tanti strati di centinaia/migliaia di neuroni i quali ognuno ha lo scopo di calcolare una sommatoria pesata dei dati che arrivano dalle varie sinapsi e passarle allo strato successivo.
        Per la fase di training verrà dato in input un grande numero di frame gia etichettati cosi che, grazie all'aiuto di specifiche tecniche e calcoli (ad esempio algoritmi specifici di backPropagation e funzioni di costo), possa aggiustare i propri parametri al fine di rinoscere al meglio la natura di ogni pixel. Per creare il dataset bisogna etichettare ogni pixel di ogni frame disponibile da video pertinenti. Per fare ciò bisogna creare una nuova immagine (una mask) da dare come etichetta all'immagine originale in cui a ogni pixel viene associata una classe di appartenenza.
        set di immagini ognuna associata ad una maschera appositamente creata in cui ad ogni posizione di pixel viene associata una particolare classe di appartenenza
        -->
    </div>
    <div id="immagine_boat" style="position:relative;width:50%;left:25%;top:20px;" class="img_didasc" >
      <img src="img_boat.jpg"/ style="width:100%">
      <p id="didasc">Fig. 1: Immagine AVS utilizzato dal progetto Intcatch.</p>
    </div>
    <div style="position:relative; top :30px;" >
      <div id="tool"> <h1>Tool di segmentazione</h1>
        <p>
          Per creare il dataset utile in questo ambito &egrave; stato usato un tool creato appositamente.<br>La <a href="#immagine_tool" style="color:orange;text-decoration:none;">Fig. 2<a> mostra un esempio di schermata del tool utilizzato.<br>
          Con questo programma &egrave; possibile segmentare l'immagine con tre diversi metodi presenti nella libreria Skimage di Python:<br>
          <ul>
            <li> <b>SLIC</b> (Simple Linear Iterative Clustering): Segmenta l'immagine usando l'algoritmo di clustering k-means nello spazio dei colori (x,y,z). </li>
            <li> <b>Felzenszwalb</b>: Produce una sovra-segmentazione di un'immagine multicanale (i.e RGB) utilizzando un clustering rapido e minimo basato su spanning tree sulla griglia dell'immagine. Il parametro sigma &egrave; il diametro di un kernel Gaussiano, usato per fare smoothing dell'immagine prima della segmentazione.</li>
            <li> <b>Quickshift</b>: Produce una sovra-segmentazione dell'immagine usando l'algoritmo di ricerca quickshift. Questo algoritmo, pur essendo molto efficacie, rende la segmentazione molto lenta.</li>
          </ul>
          Per una spiegazione dettagliata all'utilizzo dei vari metodi utilizzati vi rimando al sito della <a href="https://scikit-image.org/docs/0.12.x/api/skimage.segmentation.html#id3" style=" text-decoration:none;color:orange">libreria Python Skimage</a>.
        </p><p>
        Una volta segmentata l'immagine &egrave; possibile etichettare le regioni di pixel con una specifica classe. Nel nostro caso sono state utilizzate 5 superclassi (animali in acqua, ostacoli in acqua, cielo, costa e acqua) per raggruppare oggetti di natura simile in questo contesto.
        Ogni etichetta &egrave; associata ad un colore (consultabili nella <a href="#legenda" style="text-decoration: none;color:orange">legenda</a>) cos&igrave; che, una volta che si vuole concludere l'operazione di labeling, viene creata una maschera di dimensioni uguali all'immagine di partenza in cui ogni pixel colorato corrisponde all'etichetta del pixel dell'immagine di partenza.<br>
        Per la creazione del datset sono stati usati video registrati al lago di Garda e sul fiume Ter in Spagna. Per ampliare il dataset e gli scenari sono state anche usate immagini pubbliche disponibili al sito <a href="" style="text-decoration:none;color:orange;">asas</a>.<br>Una parte delle immagini del dataset con le corrispondenti maschere sono consultabili nella sezione <a href="#dataset" style="text-decoration: none;color:orange">Dataset</a> con la possibilit&agrave; di fare Download.
      </p><p>Durante l'addestramento della rete &egrave; possibile anche lavorare sul dataset con l'operazione di data Augmentation. Questa tecnica permette di aumentare artificialmente le dimensioni del dataset usando operazioni come insversione, rotazione e cambiamenti di luminosit&agrave; sulle sottoimmagini dell'immagine originale. L'ampliamento del dataset pu&ograve; aumentare la capacit&agrave; di generalizzazione della rete.
      </div>
      <div id="prova" >
        <div id="immagine_tool" class="img_didasc">
          <img style="position:relative;width:100%;" src="tool.png"/>
          <p id="didasc">Fig. 2: Esempio di schermata Tool di segmentazione.</p>
        </div>
        <div id="legenda"> <b>LEGENDA COLORI</b>:
          <dl>
            <dt><span class="dot1" style="position:absolute;height:10px;width:10px;background-color:#6464FF;display:inline;border-radius:50%"><dd>acqua</dd></span></td>

            <dt><span class="dot2" style="position:absolute;height:10px;width:10px;background-color:#00FF00;display:inline;border-radius:50%"></span></dt>
              <dd>costa</dd>
            <dt><span class="dot3" style="position:absolute;height:10px;width:10px;background-color:#FF00FF;display:inline;border-radius:50%"></span></dt>
              <dd>cielo</dd>
            <dt><span class="dot4" style="position:absolute;height:10px;width:10px;background-color:#FF0000;display:inline;border-radius:50%"></span></dt>
              <dd>ostacoli in acqua</dd>
            <dt><span class="dot5" style="position:absolute;height:10px;width:10px;background-color:#C8C814;display:inline;border-radius:50%"></span></dt>
              <dd>animali in acqua</dd>
          </dl>
        </div>
      </div>
    </div>
    <div class="box" id ="dataset">
      <h1>Dataset</h1>
    </div>
    <div class="box">
      <button id="bottoneDownload" onclick="Download()">Download</button>
    </div>
    <div class="box">
      <select id="selectvideo" onchange="changevideo()">
      </select>
      <select id="selectimage" onchange="changeimage()">
      </select>
      <a href="javascript:inizialization()" class="testo">reset</a>
    </div>
    <div id="box_immagini" class="item animated zoomIn">
      <div id="im" ><img id="immagine" class="immagine"> <a id="bottone1" onclick="indietro()"></a></div>
      <div id="im1"><img id="immagine1" class="immagine" > <a id="bottone2"onclick="avanti()"></a> </div>
    </div>
    <div style="position :relative;"></div>

  <div style="position:relative;top:50px;color:white;padding-left:5%;">

    <p>Una volta completata la creazione del dataset, quest'ultimo deve essere suddiviso casualmente in 3 cartelle. Ogni dataset servir&agrave; per una determinata fase di creazione di una rete neurale. Le tre fasi sono:
      <ul><li><b>Training</b></li>
        <li><b>Validation</b></li>
        <li><b>Testing</b></li>
      </ul>
      Prima di vedere il funzionamento della rete andiamo ad analizzarne l'architettura.
    </p>
  </div>
  <div  id="archi_rete" style="position:relative; top:20px;color:white;padding-left:5%;padding-right:5%;">
    <h1>Architettura Rete</h1>
    <p style="color:white;">Come architettura di rete &egrave; stata usata una CNN (Convolutional Neural Network). Questo tipo di architettura mostra buone prestazioni per la classificazione di immagini. Pi&ugrave; precisamente viene utilizzata un'architettura che discende da U-net per la segmentazione delle immagini.<br>
      U-net &egrave; stata sviluppata da Olaf Ronneberger et al. per la Bio Image Segmentation. L'architettura contiene due fasi. La prima &egrave; chiamta di "encoding" e serve per acquisire il contesto nell'immagine riducendone le dimensioni. Questo percorso &egrave; una successione tradizionale di strati convoluzionali e di max-pooling. La seconda fase &egrave; chiamata decoding e viene utilizzata per abilitare la localizzazione precisa utilizzando convoluzioni trasposte e ricomponendo le dimensioni dell'immagine originale. Questo tipo di rete  &egrave; completamente convoluzionale (FCN) end-to-end, cio &egrave; contiene solo livelli Convolutional e non contiene alcun livello Dense con conseguente assenza di restrizioni sulle dimensioni dell'immagini in ingresso.
    </p><p>
      La <a href="#immagine_archi" style="color:orange;text-decoration:none;">Fig. 3</a> presenta la U-net modificata utilizzata, chiamata anche Full BN 160&#x2715;160.<br>
      A differenza della U-net originale lo strato di codifica utilizzato &egrave; composto da quattro livelli anzich&egrave; cinque e vengono estratte la met&agrave; delle feature per ogni livello. La prima fase &egrave; composta da otto strati convoluzionali 3&#x2715;3 e da tre operazioni di max pooling 2&#x2715;2 con stride 2. La fase di espansione &egrave; costituita da sei strati convoluzionali 3&#x2715;3 e da tre strati di trasposizione 2&#x2715;2. La mappa a 32 canali estratta nell'ultimo passo della rete viene proiettata in uno spazio a singolo canale tramite una funzione di attivazione sigmoide per generare la probabilit&agrave; di appartenenza alla classe "acqua" per ogni signolo pixel.<br>
       In questa architettura &egrave; stata introdotta anche una funzione chiamata Batch Normalization. SPIEGAZIONE BATCH NORMALIZATION  .
     </p><p>
      Oltre all'architettura Full BN 160&#x2715;160 sono state proposte altre configurazioni. L'architettura infatti deve essere in grado di elaborare i dati con gran velocit&agrave; per riuscire a riconoscere ostacoli in un tempo accettabile. Questo il motivo per cui sono state introdotte altre tipi di architettura chiamate Half-Conv BN 160&#x2715;160, Full mobile-net-v1-layer 160&#x2715;160 e Full mobile-net -v2-layer 160&#x2715;160.
      Half-Conv BN 160&#x2715;160 contiene la met&agrave; dei livelli convoluzionali della rete Full BN. Riducendo i livelli convoluzionali a un singolo livello per codifica/decodifica, è possibile ottenere un calcolo più rapido in termini di FPS a scapito della precisione che vede un peggioramento. Inoltre, togliendo la BN, è possibile aumentarne la velocità sempre a discapito della precisione di rilevazione. Ci riferiamo alle due versioni della rete senza BN come Full 160&#x2715;160 e Half-Conv 160&#x2715;160 , rispettivamente. Le reti Full mobile-net-v1-layer 160&#x2715;160 Full mobile-net -v2-layer 160&#x2715;160 derivano da MobileNet. MobileNet è un tipo di rete dove gli strati convoluzionali tradizionali dal livello 1 al 4 sono sostituiti da convoluzioni depthwise separable (DSC). Questo tipo di rete è apposita per dispositivi mobili perchè rende il rilevamento più veloce.
    </p>
    </div>
  <div id="immagine_archi" style="position:relative;width:50%;left:25%;top:20px;" class="img_didasc" >
    <img src="architettura.jpg"/ style="width:100%">
    <p id="didasc">Fig. 3: Architettura U-net Full BN 160 x 160.</p>
  </div>
  <div id="addestramento" style="position:relative;top:20px;color:white;padding-left:5%; padding-right:5%;">
    <h1>Training</h1>
    <p>
      Per addestrare la rete neurale bisogna utilizzare il dataset in cui ad ogni immagine &egrave; associata una mask che ne indica le classi di appartenenza dei pixel. Inizialmente i parametri della rete sono impostati in modo casuale. Spetter&agrave; alla fase di training aggiustare questi parametri al fine di completare la funzionalit&agrave; richiestra. Per questo motivo bisogna impostare un algoritmo di ottimizzazione e una funzione di costo.
      La funzione di costo ha il compito di calcolare la distanza tra la predizione della rete e la maschera associata all'immagine di input. Una volta calcolata la funzione di costo &egrave; possibile utilizzare algoritmi di ottimizzazione che, utilizzandone il gradiente, bilanciano i pesi in modo da minimizzare l'errore.<br>
      In questo caso la loss function &egrave; stata trovata sperimentalmente tramite un confronto tra varie funzioni. La funzione che ha dato risultati migliori (come mostrato in <a href="#immagine_loss" style="text-decoration:none;color:orange" >Fig. 4</a>) &egrave; stata Dice Similarity Coefficient (DSC) definita come:
      $$J = {\scr L}_{DSC} = 1 - \frac{2\sum\nolimits_{i}^Np_ig_i}{\sum\nolimits_{i}^Np_i+\sum\nolimits_{i}^Ng_i}$$
      dove \(p_i,g_i&#x2208;[0,1]\) rappresentano rispettivamente i valori continui della griglia di predizione sigmoide e il vero valore appertenente alla maschera, per ogni pixel \(i\). <br>
      Per quanto riguarda l'algoritmo di ottimizzazione Nel nostro caso &egrave; stato usato la discesa lungo gradiente con Mini-batch. Un mini-batch &egrave; un sottoinsieme di immagini di dimensione \(m\) del dataset originale. Per ogni mini-batch, quindi, l'algoritmo calcola il gradiente della funzione costo. L'ottimizzazione su mini-batch utilizza la media ponderata dei gradienti calcolati all'interno del singolo batch per muovere i pesi alla ricerca del minimo della funzione risultante .
      Quindi la funzione calcolata su un mini-batch risulta essere una media del tipo:
      $$J(\theta,x_{(z:z+m)},y_{(z:z+m)})=\frac{1}{m}\sum\nolimits_{z=0}^mJ(\theta,x_{(z)},y_{(z)})$$
      Come ottimizzatore &egrave; stato usato Adam (Adaptive moment estimation) con batch size di 32 immagini. L'ottimizzatore Adam calcola la discesa lungo gradiente con momento. Il conseguente aggiornamento dei pesi pu&ograve; essere scritto come:
      $$\theta=\theta-\alpha\Delta J(\theta;x_{(z:z+m)};y_{(z:z+m)})$$
      dove a \(\theta\), definito come singolo peso del neurone, viene sottratto il gradiente \(\Delta\),calcolato tramite l'ottimizzatore, moltiplicato per un fattore \(\alpha\) chiamato learning Rate. Nel nostro caso \(\alpha\) &egrave; uguale a \(10^{-4}\).
      </p>
      <p>
        Il training della rete &egrave; stato fatto per 100 epoche. Per evitare l'overfitting &egrave; stato usata l'interruzione anticipata con patient 20 basata sulla loss function rispetto al validation set.
      </p>
  </div>

  <div id="immagine_loss" style="position:relative;width:50%;left:25%;top:20px;" class="img_didasc" >
    <img src="img_loss.jpg"/ style="width:100%">
    <p id="didasc">Fig. 4: Confronto delle varie funzioni loss tramite metrica Dice/F1.</p>
  </div>
  <div id="Testing" style="position:relative;top:20px;color:white;padding-left:5%;padding-right:5%;">
    <h1>Testing</h1>
    <p>
      Al fine di testare la rete la
    </p>
  </div>
  <div class="box"><br><br></div>
  <a id="back_to_top"></a>
  <div id="postMenu"><div id="aperto"><div style="position:relative;width:100%; height:50px;"><h3>INDICE</h3></div><br>
                <a href="#Intro" style="text-decoration:none;color:black;">INTRODUZIONE</a><br>
                <a href="#tool" style="text-decoration:none;color:black">TOOL DI SEGMENTAZIONE</a><br>
                <a href="#dataset" style="text-decoration:none;color:black">DATASET</a><br>
                <a href="#archi_rete" style="text-decoration:none;color:black">ARCHITETTURA RETE</a><br>
                <a href="#addestramento" style="text-decoration:none;color:black">TRAINING</a><br>
                <a href="#Testing" style="text-decoration:none;color:black">TESTING</a><br><br>
              </div></div>

  </body>
</html>
